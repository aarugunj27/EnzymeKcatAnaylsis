{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1695906432748537,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023391812865497075,
      "grad_norm": 7.944921970367432,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 25.4395,
      "step": 10
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 24.816808700561523,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 25.0403,
      "step": 20
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 9.938749313354492,
      "learning_rate": 3e-06,
      "loss": 30.0867,
      "step": 30
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 26.820388793945312,
      "learning_rate": 4.000000000000001e-06,
      "loss": 28.6068,
      "step": 40
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 4.796237945556641,
      "learning_rate": 5e-06,
      "loss": 29.0696,
      "step": 50
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 6.756719589233398,
      "learning_rate": 6e-06,
      "loss": 26.3627,
      "step": 60
    },
    {
      "epoch": 0.16374269005847952,
      "grad_norm": 4.061956882476807,
      "learning_rate": 7.000000000000001e-06,
      "loss": 29.8315,
      "step": 70
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 13.271060943603516,
      "learning_rate": 8.000000000000001e-06,
      "loss": 25.2,
      "step": 80
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 17.327655792236328,
      "learning_rate": 9e-06,
      "loss": 32.4491,
      "step": 90
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 18.36493492126465,
      "learning_rate": 1e-05,
      "loss": 38.9101,
      "step": 100
    },
    {
      "epoch": 0.2573099415204678,
      "grad_norm": 14.24201774597168,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 22.7999,
      "step": 110
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 14.532678604125977,
      "learning_rate": 1.2e-05,
      "loss": 27.0763,
      "step": 120
    },
    {
      "epoch": 0.30409356725146197,
      "grad_norm": 13.377220153808594,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 29.5261,
      "step": 130
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 15.767023086547852,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 29.4029,
      "step": 140
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 25.234453201293945,
      "learning_rate": 1.5e-05,
      "loss": 23.4148,
      "step": 150
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 18.635671615600586,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 30.7628,
      "step": 160
    },
    {
      "epoch": 0.39766081871345027,
      "grad_norm": 36.94607162475586,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 24.8727,
      "step": 170
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 17.313066482543945,
      "learning_rate": 1.8e-05,
      "loss": 23.1253,
      "step": 180
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 25.824806213378906,
      "learning_rate": 1.9e-05,
      "loss": 28.9646,
      "step": 190
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 19.53084945678711,
      "learning_rate": 2e-05,
      "loss": 25.2401,
      "step": 200
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 29.001728057861328,
      "learning_rate": 2.1e-05,
      "loss": 26.6727,
      "step": 210
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 14.269728660583496,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 31.3958,
      "step": 220
    },
    {
      "epoch": 0.5380116959064327,
      "grad_norm": 54.28510284423828,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 27.4707,
      "step": 230
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 51.88645553588867,
      "learning_rate": 2.4e-05,
      "loss": 21.691,
      "step": 240
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 41.04386520385742,
      "learning_rate": 2.5e-05,
      "loss": 18.5536,
      "step": 250
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 70.38684844970703,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 24.949,
      "step": 260
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 42.482208251953125,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 24.5665,
      "step": 270
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 32.338008880615234,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 23.4265,
      "step": 280
    },
    {
      "epoch": 0.6783625730994152,
      "grad_norm": 44.25801086425781,
      "learning_rate": 2.9e-05,
      "loss": 26.7232,
      "step": 290
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 96.5752944946289,
      "learning_rate": 3e-05,
      "loss": 22.7235,
      "step": 300
    },
    {
      "epoch": 0.7251461988304093,
      "grad_norm": 42.498008728027344,
      "learning_rate": 3.1e-05,
      "loss": 19.4906,
      "step": 310
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 49.527000427246094,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 19.9489,
      "step": 320
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 98.06372833251953,
      "learning_rate": 3.3e-05,
      "loss": 23.3134,
      "step": 330
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 129.74591064453125,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 22.8017,
      "step": 340
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 58.073970794677734,
      "learning_rate": 3.5e-05,
      "loss": 24.4846,
      "step": 350
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 221.04713439941406,
      "learning_rate": 3.6e-05,
      "loss": 17.1369,
      "step": 360
    },
    {
      "epoch": 0.8654970760233918,
      "grad_norm": 85.14692687988281,
      "learning_rate": 3.7e-05,
      "loss": 27.4532,
      "step": 370
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 79.62397003173828,
      "learning_rate": 3.8e-05,
      "loss": 21.3848,
      "step": 380
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 83.9798355102539,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 20.1053,
      "step": 390
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 146.5889129638672,
      "learning_rate": 4e-05,
      "loss": 27.3471,
      "step": 400
    },
    {
      "epoch": 0.9590643274853801,
      "grad_norm": 99.75618743896484,
      "learning_rate": 4.1e-05,
      "loss": 25.4884,
      "step": 410
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 130.75979614257812,
      "learning_rate": 4.2e-05,
      "loss": 21.5855,
      "step": 420
    },
    {
      "epoch": 0.9988304093567252,
      "eval_loss": 23.969139099121094,
      "eval_runtime": 12.8078,
      "eval_samples_per_second": 38.882,
      "eval_steps_per_second": 9.76,
      "step": 427
    },
    {
      "epoch": 1.0058479532163742,
      "grad_norm": 143.44793701171875,
      "learning_rate": 4.3e-05,
      "loss": 24.8012,
      "step": 430
    },
    {
      "epoch": 1.0292397660818713,
      "grad_norm": 201.185791015625,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 21.0955,
      "step": 440
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 33.66645431518555,
      "learning_rate": 4.5e-05,
      "loss": 21.5775,
      "step": 450
    },
    {
      "epoch": 1.0760233918128654,
      "grad_norm": 108.16558837890625,
      "learning_rate": 4.600000000000001e-05,
      "loss": 17.2285,
      "step": 460
    },
    {
      "epoch": 1.0994152046783625,
      "grad_norm": 16.641645431518555,
      "learning_rate": 4.7e-05,
      "loss": 17.2361,
      "step": 470
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 234.79385375976562,
      "learning_rate": 4.8e-05,
      "loss": 24.0422,
      "step": 480
    },
    {
      "epoch": 1.1461988304093567,
      "grad_norm": 144.35374450683594,
      "learning_rate": 4.9e-05,
      "loss": 24.7411,
      "step": 490
    },
    {
      "epoch": 1.1695906432748537,
      "grad_norm": 286.2016296386719,
      "learning_rate": 5e-05,
      "loss": 14.9779,
      "step": 500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1281,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 46072313329152.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
