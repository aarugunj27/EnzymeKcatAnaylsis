{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9964912280701754,
  "eval_steps": 500,
  "global_step": 1281,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023391812865497075,
      "grad_norm": 7.944921970367432,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 25.4395,
      "step": 10
    },
    {
      "epoch": 0.04678362573099415,
      "grad_norm": 24.816808700561523,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 25.0403,
      "step": 20
    },
    {
      "epoch": 0.07017543859649122,
      "grad_norm": 9.938749313354492,
      "learning_rate": 3e-06,
      "loss": 30.0867,
      "step": 30
    },
    {
      "epoch": 0.0935672514619883,
      "grad_norm": 26.820388793945312,
      "learning_rate": 4.000000000000001e-06,
      "loss": 28.6068,
      "step": 40
    },
    {
      "epoch": 0.11695906432748537,
      "grad_norm": 4.796237945556641,
      "learning_rate": 5e-06,
      "loss": 29.0696,
      "step": 50
    },
    {
      "epoch": 0.14035087719298245,
      "grad_norm": 6.756719589233398,
      "learning_rate": 6e-06,
      "loss": 26.3627,
      "step": 60
    },
    {
      "epoch": 0.16374269005847952,
      "grad_norm": 4.061956882476807,
      "learning_rate": 7.000000000000001e-06,
      "loss": 29.8315,
      "step": 70
    },
    {
      "epoch": 0.1871345029239766,
      "grad_norm": 13.271060943603516,
      "learning_rate": 8.000000000000001e-06,
      "loss": 25.2,
      "step": 80
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 17.327655792236328,
      "learning_rate": 9e-06,
      "loss": 32.4491,
      "step": 90
    },
    {
      "epoch": 0.23391812865497075,
      "grad_norm": 18.36493492126465,
      "learning_rate": 1e-05,
      "loss": 38.9101,
      "step": 100
    },
    {
      "epoch": 0.2573099415204678,
      "grad_norm": 14.24201774597168,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 22.7999,
      "step": 110
    },
    {
      "epoch": 0.2807017543859649,
      "grad_norm": 14.532678604125977,
      "learning_rate": 1.2e-05,
      "loss": 27.0763,
      "step": 120
    },
    {
      "epoch": 0.30409356725146197,
      "grad_norm": 13.377220153808594,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 29.5261,
      "step": 130
    },
    {
      "epoch": 0.32748538011695905,
      "grad_norm": 15.767023086547852,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 29.4029,
      "step": 140
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 25.234453201293945,
      "learning_rate": 1.5e-05,
      "loss": 23.4148,
      "step": 150
    },
    {
      "epoch": 0.3742690058479532,
      "grad_norm": 18.635671615600586,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 30.7628,
      "step": 160
    },
    {
      "epoch": 0.39766081871345027,
      "grad_norm": 36.94607162475586,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 24.8727,
      "step": 170
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 17.313066482543945,
      "learning_rate": 1.8e-05,
      "loss": 23.1253,
      "step": 180
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 25.824806213378906,
      "learning_rate": 1.9e-05,
      "loss": 28.9646,
      "step": 190
    },
    {
      "epoch": 0.4678362573099415,
      "grad_norm": 19.53084945678711,
      "learning_rate": 2e-05,
      "loss": 25.2401,
      "step": 200
    },
    {
      "epoch": 0.49122807017543857,
      "grad_norm": 29.001728057861328,
      "learning_rate": 2.1e-05,
      "loss": 26.6727,
      "step": 210
    },
    {
      "epoch": 0.5146198830409356,
      "grad_norm": 14.269728660583496,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 31.3958,
      "step": 220
    },
    {
      "epoch": 0.5380116959064327,
      "grad_norm": 54.28510284423828,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 27.4707,
      "step": 230
    },
    {
      "epoch": 0.5614035087719298,
      "grad_norm": 51.88645553588867,
      "learning_rate": 2.4e-05,
      "loss": 21.691,
      "step": 240
    },
    {
      "epoch": 0.5847953216374269,
      "grad_norm": 41.04386520385742,
      "learning_rate": 2.5e-05,
      "loss": 18.5536,
      "step": 250
    },
    {
      "epoch": 0.6081871345029239,
      "grad_norm": 70.38684844970703,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 24.949,
      "step": 260
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 42.482208251953125,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 24.5665,
      "step": 270
    },
    {
      "epoch": 0.6549707602339181,
      "grad_norm": 32.338008880615234,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 23.4265,
      "step": 280
    },
    {
      "epoch": 0.6783625730994152,
      "grad_norm": 44.25801086425781,
      "learning_rate": 2.9e-05,
      "loss": 26.7232,
      "step": 290
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 96.5752944946289,
      "learning_rate": 3e-05,
      "loss": 22.7235,
      "step": 300
    },
    {
      "epoch": 0.7251461988304093,
      "grad_norm": 42.498008728027344,
      "learning_rate": 3.1e-05,
      "loss": 19.4906,
      "step": 310
    },
    {
      "epoch": 0.7485380116959064,
      "grad_norm": 49.527000427246094,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 19.9489,
      "step": 320
    },
    {
      "epoch": 0.7719298245614035,
      "grad_norm": 98.06372833251953,
      "learning_rate": 3.3e-05,
      "loss": 23.3134,
      "step": 330
    },
    {
      "epoch": 0.7953216374269005,
      "grad_norm": 129.74591064453125,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 22.8017,
      "step": 340
    },
    {
      "epoch": 0.8187134502923976,
      "grad_norm": 58.073970794677734,
      "learning_rate": 3.5e-05,
      "loss": 24.4846,
      "step": 350
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 221.04713439941406,
      "learning_rate": 3.6e-05,
      "loss": 17.1369,
      "step": 360
    },
    {
      "epoch": 0.8654970760233918,
      "grad_norm": 85.14692687988281,
      "learning_rate": 3.7e-05,
      "loss": 27.4532,
      "step": 370
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 79.62397003173828,
      "learning_rate": 3.8e-05,
      "loss": 21.3848,
      "step": 380
    },
    {
      "epoch": 0.9122807017543859,
      "grad_norm": 83.9798355102539,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 20.1053,
      "step": 390
    },
    {
      "epoch": 0.935672514619883,
      "grad_norm": 146.5889129638672,
      "learning_rate": 4e-05,
      "loss": 27.3471,
      "step": 400
    },
    {
      "epoch": 0.9590643274853801,
      "grad_norm": 99.75618743896484,
      "learning_rate": 4.1e-05,
      "loss": 25.4884,
      "step": 410
    },
    {
      "epoch": 0.9824561403508771,
      "grad_norm": 130.75979614257812,
      "learning_rate": 4.2e-05,
      "loss": 21.5855,
      "step": 420
    },
    {
      "epoch": 0.9988304093567252,
      "eval_loss": 23.969139099121094,
      "eval_runtime": 12.8078,
      "eval_samples_per_second": 38.882,
      "eval_steps_per_second": 9.76,
      "step": 427
    },
    {
      "epoch": 1.0058479532163742,
      "grad_norm": 143.44793701171875,
      "learning_rate": 4.3e-05,
      "loss": 24.8012,
      "step": 430
    },
    {
      "epoch": 1.0292397660818713,
      "grad_norm": 201.185791015625,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 21.0955,
      "step": 440
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 33.66645431518555,
      "learning_rate": 4.5e-05,
      "loss": 21.5775,
      "step": 450
    },
    {
      "epoch": 1.0760233918128654,
      "grad_norm": 108.16558837890625,
      "learning_rate": 4.600000000000001e-05,
      "loss": 17.2285,
      "step": 460
    },
    {
      "epoch": 1.0994152046783625,
      "grad_norm": 16.641645431518555,
      "learning_rate": 4.7e-05,
      "loss": 17.2361,
      "step": 470
    },
    {
      "epoch": 1.1228070175438596,
      "grad_norm": 234.79385375976562,
      "learning_rate": 4.8e-05,
      "loss": 24.0422,
      "step": 480
    },
    {
      "epoch": 1.1461988304093567,
      "grad_norm": 144.35374450683594,
      "learning_rate": 4.9e-05,
      "loss": 24.7411,
      "step": 490
    },
    {
      "epoch": 1.1695906432748537,
      "grad_norm": 286.2016296386719,
      "learning_rate": 5e-05,
      "loss": 14.9779,
      "step": 500
    },
    {
      "epoch": 1.1929824561403508,
      "grad_norm": 126.73300170898438,
      "learning_rate": 4.935979513444303e-05,
      "loss": 21.9653,
      "step": 510
    },
    {
      "epoch": 1.2163742690058479,
      "grad_norm": 137.03602600097656,
      "learning_rate": 4.871959026888605e-05,
      "loss": 17.7251,
      "step": 520
    },
    {
      "epoch": 1.239766081871345,
      "grad_norm": 198.81021118164062,
      "learning_rate": 4.807938540332907e-05,
      "loss": 18.8161,
      "step": 530
    },
    {
      "epoch": 1.263157894736842,
      "grad_norm": 120.61664581298828,
      "learning_rate": 4.743918053777209e-05,
      "loss": 21.8748,
      "step": 540
    },
    {
      "epoch": 1.286549707602339,
      "grad_norm": 183.9237518310547,
      "learning_rate": 4.6798975672215114e-05,
      "loss": 21.3267,
      "step": 550
    },
    {
      "epoch": 1.3099415204678362,
      "grad_norm": 267.1841125488281,
      "learning_rate": 4.615877080665813e-05,
      "loss": 22.8917,
      "step": 560
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 121.578857421875,
      "learning_rate": 4.551856594110115e-05,
      "loss": 25.0952,
      "step": 570
    },
    {
      "epoch": 1.3567251461988303,
      "grad_norm": 85.46697998046875,
      "learning_rate": 4.487836107554418e-05,
      "loss": 19.0455,
      "step": 580
    },
    {
      "epoch": 1.3801169590643274,
      "grad_norm": 88.8626480102539,
      "learning_rate": 4.42381562099872e-05,
      "loss": 19.2094,
      "step": 590
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 146.31460571289062,
      "learning_rate": 4.359795134443022e-05,
      "loss": 26.1975,
      "step": 600
    },
    {
      "epoch": 1.4269005847953216,
      "grad_norm": 100.58677673339844,
      "learning_rate": 4.295774647887324e-05,
      "loss": 20.763,
      "step": 610
    },
    {
      "epoch": 1.4502923976608186,
      "grad_norm": 351.2084655761719,
      "learning_rate": 4.2317541613316264e-05,
      "loss": 17.9749,
      "step": 620
    },
    {
      "epoch": 1.4736842105263157,
      "grad_norm": 272.96734619140625,
      "learning_rate": 4.167733674775928e-05,
      "loss": 18.9867,
      "step": 630
    },
    {
      "epoch": 1.4970760233918128,
      "grad_norm": 193.78985595703125,
      "learning_rate": 4.103713188220231e-05,
      "loss": 17.141,
      "step": 640
    },
    {
      "epoch": 1.52046783625731,
      "grad_norm": 176.04139709472656,
      "learning_rate": 4.039692701664533e-05,
      "loss": 28.2766,
      "step": 650
    },
    {
      "epoch": 1.543859649122807,
      "grad_norm": 33.65457534790039,
      "learning_rate": 3.975672215108835e-05,
      "loss": 19.4575,
      "step": 660
    },
    {
      "epoch": 1.5672514619883042,
      "grad_norm": 182.7753448486328,
      "learning_rate": 3.9116517285531375e-05,
      "loss": 18.3365,
      "step": 670
    },
    {
      "epoch": 1.590643274853801,
      "grad_norm": 97.24749755859375,
      "learning_rate": 3.8476312419974394e-05,
      "loss": 18.2385,
      "step": 680
    },
    {
      "epoch": 1.6140350877192984,
      "grad_norm": 130.60855102539062,
      "learning_rate": 3.7836107554417414e-05,
      "loss": 19.9704,
      "step": 690
    },
    {
      "epoch": 1.6374269005847952,
      "grad_norm": 128.3375701904297,
      "learning_rate": 3.719590268886043e-05,
      "loss": 21.2541,
      "step": 700
    },
    {
      "epoch": 1.6608187134502925,
      "grad_norm": 350.25390625,
      "learning_rate": 3.655569782330346e-05,
      "loss": 19.7203,
      "step": 710
    },
    {
      "epoch": 1.6842105263157894,
      "grad_norm": 270.2000427246094,
      "learning_rate": 3.5915492957746486e-05,
      "loss": 18.3451,
      "step": 720
    },
    {
      "epoch": 1.7076023391812867,
      "grad_norm": 310.7485656738281,
      "learning_rate": 3.52752880921895e-05,
      "loss": 23.9198,
      "step": 730
    },
    {
      "epoch": 1.7309941520467835,
      "grad_norm": 163.1565399169922,
      "learning_rate": 3.4635083226632525e-05,
      "loss": 21.2631,
      "step": 740
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 109.40483856201172,
      "learning_rate": 3.3994878361075544e-05,
      "loss": 24.6124,
      "step": 750
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 430.5940246582031,
      "learning_rate": 3.335467349551857e-05,
      "loss": 16.6927,
      "step": 760
    },
    {
      "epoch": 1.801169590643275,
      "grad_norm": 209.46847534179688,
      "learning_rate": 3.2714468629961584e-05,
      "loss": 18.2293,
      "step": 770
    },
    {
      "epoch": 1.8245614035087718,
      "grad_norm": 66.51976776123047,
      "learning_rate": 3.207426376440461e-05,
      "loss": 19.1523,
      "step": 780
    },
    {
      "epoch": 1.8479532163742691,
      "grad_norm": 156.26333618164062,
      "learning_rate": 3.1434058898847636e-05,
      "loss": 14.9052,
      "step": 790
    },
    {
      "epoch": 1.871345029239766,
      "grad_norm": 102.18340301513672,
      "learning_rate": 3.0793854033290656e-05,
      "loss": 18.1722,
      "step": 800
    },
    {
      "epoch": 1.8947368421052633,
      "grad_norm": 99.53131866455078,
      "learning_rate": 3.015364916773368e-05,
      "loss": 16.1472,
      "step": 810
    },
    {
      "epoch": 1.9181286549707601,
      "grad_norm": 112.19441223144531,
      "learning_rate": 2.9513444302176695e-05,
      "loss": 16.0222,
      "step": 820
    },
    {
      "epoch": 1.9415204678362574,
      "grad_norm": 141.89376831054688,
      "learning_rate": 2.887323943661972e-05,
      "loss": 16.6007,
      "step": 830
    },
    {
      "epoch": 1.9649122807017543,
      "grad_norm": 40.17888641357422,
      "learning_rate": 2.8233034571062744e-05,
      "loss": 20.2239,
      "step": 840
    },
    {
      "epoch": 1.9883040935672516,
      "grad_norm": 84.05905151367188,
      "learning_rate": 2.7592829705505763e-05,
      "loss": 17.3013,
      "step": 850
    },
    {
      "epoch": 2.0,
      "eval_loss": 20.531082153320312,
      "eval_runtime": 12.1479,
      "eval_samples_per_second": 40.995,
      "eval_steps_per_second": 10.29,
      "step": 855
    },
    {
      "epoch": 2.0116959064327484,
      "grad_norm": 174.3936767578125,
      "learning_rate": 2.6952624839948786e-05,
      "loss": 20.7898,
      "step": 860
    },
    {
      "epoch": 2.0350877192982457,
      "grad_norm": 116.5971908569336,
      "learning_rate": 2.6312419974391806e-05,
      "loss": 14.8211,
      "step": 870
    },
    {
      "epoch": 2.0584795321637426,
      "grad_norm": 106.99750518798828,
      "learning_rate": 2.567221510883483e-05,
      "loss": 16.1415,
      "step": 880
    },
    {
      "epoch": 2.08187134502924,
      "grad_norm": 160.72509765625,
      "learning_rate": 2.5032010243277848e-05,
      "loss": 17.2764,
      "step": 890
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 347.81170654296875,
      "learning_rate": 2.439180537772087e-05,
      "loss": 20.3716,
      "step": 900
    },
    {
      "epoch": 2.128654970760234,
      "grad_norm": 159.586669921875,
      "learning_rate": 2.3751600512163894e-05,
      "loss": 14.1965,
      "step": 910
    },
    {
      "epoch": 2.152046783625731,
      "grad_norm": 277.9580993652344,
      "learning_rate": 2.3111395646606914e-05,
      "loss": 13.5238,
      "step": 920
    },
    {
      "epoch": 2.175438596491228,
      "grad_norm": 120.6489486694336,
      "learning_rate": 2.2471190781049936e-05,
      "loss": 12.7679,
      "step": 930
    },
    {
      "epoch": 2.198830409356725,
      "grad_norm": 136.86572265625,
      "learning_rate": 2.1830985915492956e-05,
      "loss": 15.3664,
      "step": 940
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 273.81231689453125,
      "learning_rate": 2.1190781049935982e-05,
      "loss": 14.3831,
      "step": 950
    },
    {
      "epoch": 2.245614035087719,
      "grad_norm": 146.29356384277344,
      "learning_rate": 2.0550576184379002e-05,
      "loss": 18.2033,
      "step": 960
    },
    {
      "epoch": 2.2690058479532165,
      "grad_norm": 198.73646545410156,
      "learning_rate": 1.9910371318822025e-05,
      "loss": 17.1575,
      "step": 970
    },
    {
      "epoch": 2.2923976608187133,
      "grad_norm": 78.94062805175781,
      "learning_rate": 1.9270166453265044e-05,
      "loss": 15.5295,
      "step": 980
    },
    {
      "epoch": 2.3157894736842106,
      "grad_norm": 252.63320922851562,
      "learning_rate": 1.8629961587708067e-05,
      "loss": 22.8848,
      "step": 990
    },
    {
      "epoch": 2.3391812865497075,
      "grad_norm": 176.61428833007812,
      "learning_rate": 1.7989756722151087e-05,
      "loss": 13.635,
      "step": 1000
    },
    {
      "epoch": 2.3625730994152048,
      "grad_norm": 95.92514038085938,
      "learning_rate": 1.7349551856594113e-05,
      "loss": 15.2582,
      "step": 1010
    },
    {
      "epoch": 2.3859649122807016,
      "grad_norm": 540.3275756835938,
      "learning_rate": 1.6709346991037132e-05,
      "loss": 18.1408,
      "step": 1020
    },
    {
      "epoch": 2.409356725146199,
      "grad_norm": 134.1212158203125,
      "learning_rate": 1.6069142125480155e-05,
      "loss": 17.7686,
      "step": 1030
    },
    {
      "epoch": 2.4327485380116958,
      "grad_norm": 265.1440124511719,
      "learning_rate": 1.5428937259923175e-05,
      "loss": 14.5689,
      "step": 1040
    },
    {
      "epoch": 2.456140350877193,
      "grad_norm": 217.5211944580078,
      "learning_rate": 1.4788732394366198e-05,
      "loss": 16.896,
      "step": 1050
    },
    {
      "epoch": 2.47953216374269,
      "grad_norm": 95.5238265991211,
      "learning_rate": 1.4148527528809219e-05,
      "loss": 14.0154,
      "step": 1060
    },
    {
      "epoch": 2.502923976608187,
      "grad_norm": 132.7325439453125,
      "learning_rate": 1.350832266325224e-05,
      "loss": 17.1323,
      "step": 1070
    },
    {
      "epoch": 2.526315789473684,
      "grad_norm": 360.3773193359375,
      "learning_rate": 1.2868117797695265e-05,
      "loss": 15.7132,
      "step": 1080
    },
    {
      "epoch": 2.5497076023391814,
      "grad_norm": 63.051048278808594,
      "learning_rate": 1.2227912932138284e-05,
      "loss": 11.1972,
      "step": 1090
    },
    {
      "epoch": 2.573099415204678,
      "grad_norm": 133.5970916748047,
      "learning_rate": 1.1587708066581307e-05,
      "loss": 17.8801,
      "step": 1100
    },
    {
      "epoch": 2.5964912280701755,
      "grad_norm": 81.51589965820312,
      "learning_rate": 1.0947503201024328e-05,
      "loss": 12.5948,
      "step": 1110
    },
    {
      "epoch": 2.6198830409356724,
      "grad_norm": 98.30207824707031,
      "learning_rate": 1.030729833546735e-05,
      "loss": 10.0805,
      "step": 1120
    },
    {
      "epoch": 2.6432748538011697,
      "grad_norm": 103.53962707519531,
      "learning_rate": 9.667093469910372e-06,
      "loss": 9.7059,
      "step": 1130
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 142.70806884765625,
      "learning_rate": 9.026888604353394e-06,
      "loss": 14.5349,
      "step": 1140
    },
    {
      "epoch": 2.690058479532164,
      "grad_norm": 126.60403442382812,
      "learning_rate": 8.386683738796415e-06,
      "loss": 17.0331,
      "step": 1150
    },
    {
      "epoch": 2.7134502923976607,
      "grad_norm": 150.85597229003906,
      "learning_rate": 7.746478873239436e-06,
      "loss": 14.3929,
      "step": 1160
    },
    {
      "epoch": 2.736842105263158,
      "grad_norm": 117.75196075439453,
      "learning_rate": 7.106274007682459e-06,
      "loss": 18.0101,
      "step": 1170
    },
    {
      "epoch": 2.760233918128655,
      "grad_norm": 191.60308837890625,
      "learning_rate": 6.46606914212548e-06,
      "loss": 17.8541,
      "step": 1180
    },
    {
      "epoch": 2.783625730994152,
      "grad_norm": 203.65426635742188,
      "learning_rate": 5.825864276568502e-06,
      "loss": 16.5467,
      "step": 1190
    },
    {
      "epoch": 2.807017543859649,
      "grad_norm": 169.7745819091797,
      "learning_rate": 5.1856594110115235e-06,
      "loss": 17.1211,
      "step": 1200
    },
    {
      "epoch": 2.8304093567251463,
      "grad_norm": 80.80423736572266,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 15.6979,
      "step": 1210
    },
    {
      "epoch": 2.853801169590643,
      "grad_norm": 87.18868255615234,
      "learning_rate": 3.905249679897567e-06,
      "loss": 15.6545,
      "step": 1220
    },
    {
      "epoch": 2.8771929824561404,
      "grad_norm": 164.3502960205078,
      "learning_rate": 3.265044814340589e-06,
      "loss": 16.8796,
      "step": 1230
    },
    {
      "epoch": 2.9005847953216373,
      "grad_norm": 88.99397277832031,
      "learning_rate": 2.624839948783611e-06,
      "loss": 11.9241,
      "step": 1240
    },
    {
      "epoch": 2.9239766081871346,
      "grad_norm": 141.36492919921875,
      "learning_rate": 1.9846350832266325e-06,
      "loss": 12.915,
      "step": 1250
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 185.07032775878906,
      "learning_rate": 1.3444302176696543e-06,
      "loss": 12.652,
      "step": 1260
    },
    {
      "epoch": 2.9707602339181287,
      "grad_norm": 277.3987121582031,
      "learning_rate": 7.042253521126761e-07,
      "loss": 13.4702,
      "step": 1270
    },
    {
      "epoch": 2.9941520467836256,
      "grad_norm": 274.1588439941406,
      "learning_rate": 6.402048655569782e-08,
      "loss": 15.4503,
      "step": 1280
    }
  ],
  "logging_steps": 10,
  "max_steps": 1281,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 118046980131840.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
